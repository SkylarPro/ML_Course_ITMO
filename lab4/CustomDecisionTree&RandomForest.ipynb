{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"DT_test.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGClrhQA9SAk"
      },
      "source": [
        "# Деревья решений"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veekMy8WRjBi"
      },
      "source": [
        "## Построение дерева"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYkVwAFiUHXj"
      },
      "source": [
        "Опишем жадный алгоритм построения бинарного дерева решений:\n",
        "1. Начинаем со всей обучающей выборки $X$, которую помещаем в корень $R_1$. \n",
        "2. Задаём функционал качества $Q(X, j, t)$ и критерий остановки. \n",
        "3. Запускаем построение из корня: $SplitNode(1, R_1)$\n",
        "\n",
        "Функция $SplitNode(m, R_m)$\n",
        "1. Если выполнен критерий остановки, то выход.\n",
        "2. Находим наилучший с точки зрения $Q$ предикат: $j, t$: $[x_j<t]$\n",
        "3. Помещаем предикат в вкршину и получаем с его помощью разбиение $X$ на две части: $R_{left} = \\lbrace x|x_j<t \\rbrace$ и $R_{right} = \\lbrace x|x_j \\geqslant t \\rbrace$\n",
        "4. Поместим $R_{left}$ и $R_{right}$ соответсвенно в левое и правое поддерево.\n",
        "5. Рекурсивно повторяем $SplitNode(left, R_{left})$ и $SplitNode(right, R_{right})$.\n",
        "\n",
        "В конце поставим в соответствие каждому листу ответ. Для задачи классификации - это самый частый среди объектов класс или вектор с долями классов (можно интерпретировать как вероятности):\n",
        "$$ c_v = \\arg \\max_{k\\in Y} \\sum_{(x_i,y_i) \\in R_v} [y_i=k]  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P6FsdBog4Ai"
      },
      "source": [
        "## Функционал качества для деревьев решений\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VAKO0aykGBD"
      },
      "source": [
        "Энтропия Шеннона для системы с N возможными состояниями определяется по формуле:\n",
        "$$H = - \\sum_{i=0}^{N} p_i\\log_2p_i $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5582B-1Fn2bw"
      },
      "source": [
        "где $p_i$ – вероятности нахождения системы в $i$-ом состоянии. \n",
        "\n",
        "Это очень важное понятие теории информации, которое позволяет оценить количество информации (степень хаоса в системе). Чем выше энтропия, тем менее упорядочена система и наоборот. С помощью энтропии мы формализуем функционал качества для разделение выборки (для задачи классификации)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbcMUd7bvk05"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AdLxP9CowTm"
      },
      "source": [
        "Код для расчёта энтропии:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk9etb2vo7fK"
      },
      "source": [
        "Здесь $y$ - это массив значений целевой переменной"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07TCw0USzLus"
      },
      "source": [
        "Энтропия – по сути степень хаоса (или неопределенности) в системе. Уменьшение энтропии называют приростом информации (information gain, IG).\n",
        "\n",
        "Обочначим $R_v$ - объекты, которые нужно разделить в помощью предиката в вершине $v$. Запишем формулу для расчёта информационного прироста:\n",
        "$$ Q = IG = H(R_v) - (H(R_{left})+H(R_{right}))$$\n",
        "\n",
        "На каждом шаге нам нужно максимизировать этот функционал качества. Как это делать? Например, так можно перебрать $t$ для выбранного $j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trEWHDoXg_p9"
      },
      "source": [
        "Предыдущая версия формулы прироста информации слишком упрощена. В работе необходимо использовать более устойчивую формулу, которая учитывает не только энтропию подмножеств, но и их размер. \n",
        "\n",
        "$$ Q = IG = H(R_v) - \\Big (\\frac{|R_{left}|} {|R_{v}|} H(R_{left})+ \\frac{|R_{right}|} {|R_{v}|} H(R_{right})\\Big)$$\n",
        "\n",
        "где, $|R_{v}|$, $|R_{left}|$ и $|R_{right}|$ - количество элементов в соответствующих множествах."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xmN6V_N1xBr"
      },
      "source": [
        "\n",
        "### Задание 4.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWFHZScF2CBF"
      },
      "source": [
        "Реализуйте алгоритм построения дерева. Должны быть отдельные функции (методы) для расчёта энтропии (уже есть), для разделения дерева (используйте `pandas`), для подсчёта функционала качества $IG$, для выбора наилучшего разделения (с учетом признакоd и порогов), для проверки критерия остановки.\n",
        "\n",
        "Для набора данных `iris` реализуйте алгоритм и минимум три из разными критерия остановки из перечисленных ниже:\n",
        "* максимальной глубины дерева = 5\n",
        "* минимального числа объектов в листе = 5\n",
        "* максимальное количество листьев в дереве = 5\n",
        "* purity (остановка, если все объекты в листе относятся к одному классу)\n",
        "\n",
        "Реализуйте функцию `predict` (на вход функции подаётся датафрейм с объектами)\n",
        "\n",
        "Оцените точность каждой модели с помощью метрики точность (`from sklearn.metrics import accuracy_score` или реализовать свою)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r5OHUgaSjt0"
      },
      "source": [
        "import numpy as np\n",
        "from functools import partial\n",
        "import sys\n",
        "import random\n",
        "import pandas as pd\n",
        "from typing import Tuple\n",
        "sys.setrecursionlimit(10000)\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class TreeDecision:\n",
        "\n",
        "    class Node:\n",
        "        def __init__(self, data, labels, perc_data, loss):\n",
        "            self.data = data\n",
        "            self.labels = labels\n",
        "            self.perc_all_data = perc_data\n",
        "            self.n_samples = data.shape[0]\n",
        "            self.label_node, self.perc_label_node = self._compute_main_label()\n",
        "\n",
        "            self.loss = loss(labels)\n",
        "\n",
        "            self.sep_val = None\n",
        "            self.sep_features = None\n",
        "            self.left = None\n",
        "            self.right = None\n",
        "\n",
        "        def _compute_main_label(self,):\n",
        "            label, counts = np.unique(self.labels, return_counts=True)\n",
        "            main_label = np.argmax(counts)\n",
        "            perc_main_labels = counts[main_label] / self.labels.shape[0]\n",
        "            return int(label[main_label]), float(perc_main_labels)\n",
        "\n",
        "    def __init__(self, criterion=\"entropy\", max_depth=5, min_samples_split=5,\n",
        "                 max_samples_leaf=5, purity=True, numbers_feature: Tuple[int] = None):\n",
        "        \"\"\"\n",
        "        criterion: str\n",
        "                  функция оптимизации\n",
        "        max_depth: int\n",
        "                  максимальной глубины дерева\n",
        "        min_samples_split: int\n",
        "                  минимального числа объектов в листе\n",
        "        max_samples_leaf: int\n",
        "                  максимальное количество листьев в дереве\n",
        "        leaf_is_one_class: bool\n",
        "                  остановка, если все объекты в листе относятся к одному классу\n",
        "        \"\"\"\n",
        "        if criterion == \"entropy\":\n",
        "            self.criterion = self._entropy\n",
        "        elif criterion == \"gini\":\n",
        "            self.criterion = self._gini\n",
        "        else:\n",
        "            raise NotImplementedError(f\"criterion choise between Entropy and Gini \\\n",
        "       {criterion} don't implemented\")\n",
        "\n",
        "        self._max_depth = max_depth\n",
        "        self._curr_msf = 0\n",
        "        self._min_samples_split = min_samples_split\n",
        "        self._max_samples_leaf = max_samples_leaf\n",
        "        self._leaf_is_one_class = purity\n",
        "        self._root = None\n",
        "        self.n_maxs = []\n",
        "        self.numbers_feature = numbers_feature\n",
        "\n",
        "    def _gini(self, y):\n",
        "        _, counts = np.unique(y, return_counts=True)\n",
        "        probabilities = counts / counts.sum()\n",
        "        gini = 1 - sum(probabilities ** 2)\n",
        "        return gini\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        _, counts = np.unique(y, return_counts=True)\n",
        "        probabilities = counts / counts.sum()\n",
        "        entropy = sum(probabilities * -np.log2(probabilities))\n",
        "        return entropy\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if self.numbers_feature is None:\n",
        "            self.numbers_feature = tuple(range(X.shape[1]))\n",
        "        self._root = self.Node(X, y, 1, loss=self.criterion)\n",
        "        self._root = self._feed_tree(self._root, -1)\n",
        "        return self\n",
        "\n",
        "    @staticmethod\n",
        "    def cutter(X, sep_val, n_features):\n",
        "        # get left and right index for value equal optim loss\n",
        "        left = np.argwhere(X[:, n_features] < sep_val)[:, 0]\n",
        "        right = np.argwhere(X[:, n_features] >= sep_val)[:, 0]\n",
        "        return left, right\n",
        "\n",
        "    def _feed_tree(self, node, curr_depth=None):\n",
        "\n",
        "        if node.data.shape[0] < self._min_samples_split or curr_depth == self._max_depth\\\n",
        "                or self._curr_msf == self._max_samples_leaf:\n",
        "            if self._curr_msf != self._max_samples_leaf:\n",
        "                self._curr_msf += 1\n",
        "            return None\n",
        "        curr_depth+=1\n",
        "\n",
        "        # compute loss for each element\n",
        "        _splitter = partial(self._splitter, y=node.labels,)\n",
        "        losses = np.apply_along_axis(_splitter, 0, node.data.copy())[:, self.numbers_feature] # оставить только n-ые фичи\n",
        "\n",
        "        # max_loss = np.max(losses)\n",
        "        # for n_max in self.n_maxs:\n",
        "        #     losses[n_max] = max_loss\n",
        "\n",
        "        # end optimization node\n",
        "        if np.max(losses) == 0 and self._leaf_is_one_class:\n",
        "            self._curr_msf += 1\n",
        "            return node\n",
        "\n",
        "        # get optimal loss index\n",
        "        min_ids = list(np.unravel_index(np.argmin(losses), losses.shape)) #n_samples, n_features\n",
        "        min_ids[1] = self.numbers_feature[int(min_ids[1])]\n",
        "        min_ids = tuple(min_ids)\n",
        "        # надо резать не по индексу лоса а взять значения по индексу лоса и ним резать массив.\n",
        "\n",
        "        node.sep_val = node.data[min_ids]\n",
        "        node.sep_features = int(min_ids[1])\n",
        "        left, right = self.cutter(node.data.copy(), node.sep_val, node.sep_features)\n",
        "        perc_data = (left.shape[0] * node.perc_all_data) / node.data.shape[0]\n",
        "\n",
        "        assert (left.shape[0] + right.shape[0]) == node.data.shape[0],\\\n",
        "            f\"left {left.shape[0]} right {right.shape[0]}, all {node.data.shape[0]}\"\n",
        "\n",
        "\n",
        "        if left.shape[0] == 0:\n",
        "            # выбиить разделяющее значение из последующего расчета лосса, иначе зацикливание\n",
        "            node.right = self.Node(node.data[right, :], node.labels[right],\n",
        "                                   node.perc_all_data - perc_data, loss=self.criterion)\n",
        "            # self.n_maxs.append(losses[min_ids])\n",
        "            node.right = self._feed_tree(node.right, curr_depth)\n",
        "        elif right.shape[0] == 0:\n",
        "            node.left = self.Node(node.data[left, :], node.labels[left],\n",
        "                                  perc_data, loss=self.criterion)\n",
        "            node.left = self._feed_tree(node.left, curr_depth)\n",
        "        else:\n",
        "            node.left = self.Node(node.data[left, :], node.labels[left],\n",
        "                                  perc_data, loss=self.criterion)\n",
        "            node.right = self.Node(node.data[right, :], node.labels[right],\n",
        "                                   node.perc_all_data - perc_data, loss=self.criterion)\n",
        "            node.left = self._feed_tree(node.left, curr_depth)\n",
        "            node.right = self._feed_tree(node.right, curr_depth)\n",
        "        return node\n",
        "\n",
        "    def _splitter(self,X, y):\n",
        "        # проблема с лоссом!\n",
        "\n",
        "        def get_loss(split_val, X):\n",
        "            left = np.argwhere(X < split_val)[:, 0]\n",
        "            right = np.argwhere(X >= split_val)[:, 0]\n",
        "            return (((left.shape[0] / y.shape[0]) * self.criterion(y[left])) + (\n",
        "                    right.shape[0] / y.shape[0]) * self.criterion(y[right]))\n",
        "\n",
        "        get_loss = partial(get_loss, X=X)\n",
        "        return np.array(list(map(get_loss, X.copy())))\n",
        "\n",
        "    def _detour_tree(self, node, X):\n",
        "        if node.loss == 0:\n",
        "            return node.label_node\n",
        "        if X[node.sep_features] < node.sep_val:\n",
        "            if node.left:\n",
        "                label = self._detour_tree(node.left, X)\n",
        "            else:\n",
        "                return node.label_node\n",
        "        else:\n",
        "            if node.right:\n",
        "                label = self._detour_tree(node.right, X)\n",
        "            else:\n",
        "                return node.label_node\n",
        "        return label\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = np.zeros(X.shape[0])\n",
        "        for idx in range(X.shape[0]):\n",
        "            pred[idx] = self._detour_tree(self._root, X[idx, :])\n",
        "        return pred\n",
        "\n",
        "class RandomForest:\n",
        "\n",
        "    def __init__(self, n_estimators=100, max_features=\"sqrt\", criterion=\"entropy\", max_depth=5, min_samples_split=5,\n",
        "                 max_samples_leaf=5, purity=True, bs_size=0.8):\n",
        "        \"\"\"\n",
        "        n_estimatorsint: int\n",
        "                  число деревьев в ансамбле\n",
        "        max_features: str\n",
        "                тип вычисления максимального количества призанков\n",
        "        criterion: str\n",
        "                  функция оптимизации\n",
        "        max_depth: int\n",
        "                  максимальной глубины дерева\n",
        "        min_samples_split: int\n",
        "                  минимального числа объектов в листе\n",
        "         max_samples_leaf: int\n",
        "                  максимальное количество листьев в дереве\n",
        "        leaf_is_one_class: bool\n",
        "                  остановка, если все объекты в листе относятся к одному классу\n",
        "        \"\"\"\n",
        "        self._n_estimators = n_estimators\n",
        "        self._max_features = max_features\n",
        "        self._criterion = criterion\n",
        "        self._max_depth = max_depth\n",
        "        self._min_samples_split = min_samples_split\n",
        "        self._max_samples_leaf = max_samples_leaf\n",
        "        self._purity = purity\n",
        "        self.forest = None\n",
        "        self._bs_size = bs_size\n",
        "\n",
        "    def bootstrap(self,X,y, n_chunks,):\n",
        "        assert X.shape[0] == y.shape[0]\n",
        "        size_chunk = int(self._bs_size*X.shape[0])\n",
        "        bs_data = []\n",
        "        for _ in range(n_chunks):\n",
        "            idxs = tuple([random.randint(0,X.shape[0] - 1) for _ in range(size_chunk)])\n",
        "            bs_data.append((X[idxs, :].copy(), y[idxs,].reshape(-1).copy()))\n",
        "        return bs_data\n",
        "\n",
        "    def _get_n_features(self,n_features):\n",
        "        if self._max_features == \"sqrt\":\n",
        "            return tuple([random.randint(0, n_features - 1) for i in range(int(np.sqrt(n_features)))])\n",
        "\n",
        "    def build_forest(self, n_features):\n",
        "        self.forest = [TreeDecision(criterion=self._criterion, max_depth=self._max_depth,\n",
        "                                    min_samples_split=self._min_samples_split, max_samples_leaf=self._max_samples_leaf,\n",
        "                                    purity=self._purity, numbers_feature=self._get_n_features(n_features),)\n",
        "                       for _ in range(self._n_estimators)]\n",
        "        return True\n",
        "\n",
        "    def fit(self,X, y):\n",
        "        data = self.bootstrap(X, y, self._n_estimators)\n",
        "        if self.build_forest(X.shape[1]):\n",
        "            for idx in tqdm(range(len(self.forest))):\n",
        "                self.forest[idx] = self.forest[idx].fit(data[idx][0], data[idx][1])\n",
        "        return self\n",
        "\n",
        "    @staticmethod\n",
        "    def choose_preds(preds_line):\n",
        "        values, counts = np.unique(preds_line, return_counts=True)\n",
        "        finally_pred = values[np.argmax(counts)]\n",
        "        return finally_pred\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = np.zeros((X.shape[0], self._n_estimators))\n",
        "        for idx,tree in enumerate(self.forest):\n",
        "            preds[:, idx] = tree.predict(X)\n",
        "        pred_final = np.apply_along_axis(self.choose_preds, 1, preds)\n",
        "        pred_all = preds\n",
        "        return pred_final, pred_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkyCjLcy_CTM"
      },
      "source": [
        "##  Случайный лес"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fKZe1FyRgCa"
      },
      "source": [
        "Опишем алгоритм случайный лес (*random forest*) и попутно разберём основные идеи:\n",
        "\n",
        "1. Зададим $N$ - число деревьев в лесу.\n",
        "2. Для каждого $n$ из $N$ сгенерируем свою выборку $X_n$. Пусть $m$ - это количество объектов в $X$. При генерации каждой $X_n$ мы будем брать объекты $m$ раз с возвращением. То есть один и тот же объект может попасть в выборку несколько раз, а какие-то объекты не попадут. (Этот способ назвается бутстрап).\n",
        "3. По каждой $X_n$ построим решающее дерево $b_n$. Обычно стараются делать глубокие деревья. В качестве критериев остановки можно использовать `max_depth` или `min_samples_leaf` (например, пока в каждом листе не окажется по одному объекту). При каждом разбиении сначала выбирается $k$ (эвристика $k = \\sqrt d$, где $d$ - это число признаков объектов из выборки $X$) случайных признаков из исходных, и оптимальное разделение выборки ищется только среди них. Обратите внимание, что мы не выбрасываем оставшиеся признаки!\n",
        "4. Итоговый алгоритм будет представлять собой результат голосования (для классификации) и среднее арифметическое (для регрессии). Модификация алгоритма предполагает учёт весов каждого отдельного слабого алгоритма в ансамбле, но в этом особо нет смысла.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18YmKXBXuPC6"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X = data[\"data\"]\n",
        "y = data[\"target\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvaalDykug7Q",
        "outputId": "08d12d7c-0929-4168-ee9a-73a179fa69e5"
      },
      "source": [
        "model = TreeDecision(criterion=\"entropy\", max_depth=100, min_samples_split=1,\n",
        "                 max_samples_leaf=100, purity=True)\n",
        "model.fit(X,y)\n",
        "pred = model.predict(X)\n",
        "print(f\"My tree {accuracy_score(pred, y)}\")\n",
        "\n",
        "\n",
        "model = TreeDecision(criterion=\"gini\", max_depth=100, min_samples_split=51,\n",
        "                 max_samples_leaf=100, purity=True)\n",
        "model.fit(X,y)\n",
        "pred = model.predict(X)\n",
        "print(f\"My tree {accuracy_score(pred, y)}\")\n",
        "\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "model.fit(X,y)\n",
        "pred = model.predict(X)\n",
        "print(f\"Box tree {accuracy_score(pred, y)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My tree 1.0\n",
            "My tree 0.6666666666666666\n",
            "Box tree 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJBQ8lc0WyrN"
      },
      "source": [
        "### Задание 4.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y594Jn04ZTCm"
      },
      "source": [
        "В качестве набора данных используйте: https://www.kaggle.com/mathchi/churn-for-bank-customers\n",
        "\n",
        "Там есть описание и примеры работы с этими данными. Если кратко, речь идёт про задачу прогнозирования оттока клиентов. Есть данные о 10 тысячах клиентов банка, часть из которых больше не являются клиентами."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be_mLbdVW2oG"
      },
      "source": [
        "Используя либо свою реализацию, либо  `DecisionTreeClassifier` с разными настройками из `sklearn.tree` реализйте алгоритм \"случайный лес\". \n",
        "\n",
        "Найдите наилучшие гиперпараметры этого алгоритма: количество деревьев, критерий остановки, функционал качества, минимальное количество объектов в листьях и другие.\n",
        "\n",
        "Нельзя использовать готовую реализацию случайного леса из `sklearn`.\n",
        "\n",
        "В подобных задачах очень важна интерпретируемость алгоритма. Попытайтесь оценить информативность признаков, т.е. ответить а вопрос, значения каких признаков являются самыми важными индикаторами того, что банк потеряет клиента."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxZzu9eDuHCT"
      },
      "source": [
        "df = pd.read_csv(\"churn.csv\").drop(columns=[\"RowNumber\", \"CustomerId\"])\n",
        "\n",
        "for ind, uniq_pay_system in enumerate(df.Surname.unique()):\n",
        "    df.Surname = df.mask(df.Surname == uniq_pay_system, ind).Surname\n",
        "\n",
        "for ind, uniq_pay_system in enumerate(df.Geography.unique()):\n",
        "    df.Geography = df.mask(df.Geography == uniq_pay_system, ind).Geography\n",
        "\n",
        "for ind, uniq_pay_system in enumerate(df.Gender.unique()):\n",
        "    df.Gender = df.mask(df.Gender == uniq_pay_system, ind).Gender\n",
        "\n",
        "X = df.drop(columns=\"Exited\").to_numpy()\n",
        "y = df[\"Exited\"].to_numpy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=41)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtVHMujVxZQK",
        "outputId": "45be82fc-0e9b-49cf-902b-5feaa89525b8"
      },
      "source": [
        "obj = RandomForest(n_estimators=25, max_features=\"sqrt\", criterion=\"entropy\", max_depth=250, min_samples_split=10,\n",
        "                 max_samples_leaf=100, purity=True, bs_size=0.01)\n",
        "\n",
        "obj.fit(X_train, y_train)\n",
        "predict,statistic = obj.predict(X_test)\n",
        "print(np.unique(predict, return_counts=True))\n",
        "print(f\"My tree {accuracy_score(predict, y_test)}\")\n",
        "\n",
        "# obj = RandomForestClassifier(n_estimators=500, max_features=\"sqrt\",  criterion=\"entropy\", max_depth=100,min_samples_split=2,min_samples_leaf=1)\n",
        "# obj.fit(X_train, y_train)\n",
        "# predict = obj.predict(X_test)\n",
        "# print(f\"Top tree {accuracy_score(predict, y_test)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:53<00:00,  2.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([0.]), array([2500]))\n",
            "My tree 0.7948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFBkyftUxd6j"
      },
      "source": [
        "tr = np.where(statistic == y_test.reshape(-1,1),1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njK7v6371JuO",
        "outputId": "1bed5f26-2f0a-4234-9e59-39037d60b886"
      },
      "source": [
        "features_tru = {idx:0 for idx in range(X_train.shape[1])}\n",
        "features_tru"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0}"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa_JfZZF1eCP"
      },
      "source": [
        "for pr in tr:\n",
        "  for idx in range(pr.shape[0]):\n",
        "    if pr[idx] == 1:\n",
        "      for n_f in obj.forest[idx].numbers_feature:\n",
        "        features_tru[n_f] += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJKCdSnB3NqH",
        "outputId": "90b2cc17-e805-48f1-f1a2-e61391e70d80"
      },
      "source": [
        "features_tru = {v:k for k,v in features_tru.items()}\n",
        "features_tru"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0,\n",
              " 1: 3923,\n",
              " 2: 7846,\n",
              " 3: 9684,\n",
              " 4: 1987,\n",
              " 5: 8955,\n",
              " 6: 7324,\n",
              " 7: 5693,\n",
              " 8: 10033,\n",
              " 9: 0,\n",
              " 10: 1987}"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61fIIluT4hjB",
        "outputId": "56003fd6-ef96-4e71-ff5c-f73d3066ac40"
      },
      "source": [
        "sorted(features_tru.items())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 9),\n",
              " (1987, 10),\n",
              " (3923, 1),\n",
              " (5693, 7),\n",
              " (7324, 6),\n",
              " (7846, 2),\n",
              " (8955, 5),\n",
              " (9684, 3),\n",
              " (10033, 8)]"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pz4vKpH4dpX"
      },
      "source": [
        "Можем сделать вывод что более важные признаки это 8 (баланс), 3 (пол),5 (время владения) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "AXzMX-QQ4-mV",
        "outputId": "ef80844c-e9f7-4c3a-b6c5-4e9eeef64863"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Surname</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Geography</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Tenure</th>\n",
              "      <th>Balance</th>\n",
              "      <th>NumOfProducts</th>\n",
              "      <th>HasCrCard</th>\n",
              "      <th>IsActiveMember</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>Exited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>619</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101348.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>608</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>112542.58</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>502</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113931.57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>699</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93826.63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>850</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>79084.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>2539</td>\n",
              "      <td>771</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>96270.64</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>606</td>\n",
              "      <td>516</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>10</td>\n",
              "      <td>57369.61</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101699.77</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>905</td>\n",
              "      <td>709</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36</td>\n",
              "      <td>7</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>42085.58</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>1783</td>\n",
              "      <td>772</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>3</td>\n",
              "      <td>75075.31</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>92888.52</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>560</td>\n",
              "      <td>792</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>4</td>\n",
              "      <td>130142.79</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>38190.78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Surname  CreditScore Geography  ... IsActiveMember  EstimatedSalary  Exited\n",
              "0          0          619         0  ...              1        101348.88       1\n",
              "1          1          608         1  ...              1        112542.58       0\n",
              "2          2          502         0  ...              0        113931.57       1\n",
              "3          3          699         0  ...              0         93826.63       0\n",
              "4          4          850         1  ...              1         79084.10       0\n",
              "...      ...          ...       ...  ...            ...              ...     ...\n",
              "9995    2539          771         0  ...              0         96270.64       0\n",
              "9996     606          516         0  ...              1        101699.77       0\n",
              "9997     905          709         0  ...              1         42085.58       1\n",
              "9998    1783          772         2  ...              0         92888.52       1\n",
              "9999     560          792         0  ...              0         38190.78       0\n",
              "\n",
              "[10000 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    }
  ]
}